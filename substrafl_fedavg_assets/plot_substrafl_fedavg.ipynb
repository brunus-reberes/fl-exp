{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Substrafl FedAvg on MNIST dataset\n",
        "\n",
        "\n",
        "This example illustrate the basic usage of Substrafl, and propose a model training by Federated Learning\n",
        "using de Federated Average strategy.\n",
        "\n",
        "It is based on `the MNIST Dataset of handwritten digits <http://yann.lecun.com/exdb/mnist/>`__.\n",
        "\n",
        "In this example, we work on **the grayscale images** of size **28x28 pixels**. The problem considered is a\n",
        "classification problem aiming to recognize the number written on each image.\n",
        "\n",
        "The objective of this example is to launch a *federated learning* experiment on two organizations, using the **FedAvg strategy** on a\n",
        "**convolutional neural network** (CNN)\n",
        "torch model.\n",
        "\n",
        "This example does not use the deployed platform of Substra and will run in local mode.\n",
        "\n",
        "**Requirements:**\n",
        "\n",
        "  - To run this example locally, please make sure to download and unzip in the same directory as this example the\n",
        "    assets needed to run it:\n",
        "\n",
        "    .. only:: builder_html or readthedocs\n",
        "\n",
        "        :download:`assets required to run this example <../../../../../tmp/substrafl_fedavg_assets.zip>`\n",
        "\n",
        "    Please ensure to have all the libraries installed, a *requirements.txt* file is included in the zip file, where\n",
        "    you can run the command: `pip install -r requirements.txt` to install them.\n",
        "\n",
        "  - **Substra** and **Substrafl** should already be installed, if not follow the instructions described here:\n",
        "    `substrafl_doc/substrafl_overview:Installation`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Client and data preparation\n",
        "***************************\n",
        "\n",
        "Imports\n",
        "=======\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Trabalho\\miniconda3\\envs\\fl\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import codecs\n",
        "import os\n",
        "import pathlib\n",
        "import sys\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "from torchvision.datasets import MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the Substra Client\n",
        "===========================\n",
        "\n",
        "We work with two different organizations, defined by their IDs. Both organizations provide a dataset. One of them will also provide the algorithm and # will register the machine learning tasks.\n",
        "\n",
        "Once these variables defined, we can create our Substra `documentation/references/sdk:Client`.\n",
        "\n",
        "This example runs in local mode, simulating a **federated learning** experiment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from substra import Client\n",
        "\n",
        "# Choose the subprocess mode to locally simulate the FL process\n",
        "N_CLIENTS = 2\n",
        "clients = [Client(backend_type=\"subprocess\") for _ in range(N_CLIENTS)]\n",
        "clients = {client.organization_info().organization_id: client for client in clients}\n",
        "\n",
        "# Store their IDs\n",
        "ORGS_ID = list(clients.keys())\n",
        "\n",
        "# The org id on which your computation tasks are registered\n",
        "ALGO_ORG_ID = ORGS_ID[1]\n",
        "\n",
        "# Create the temporary directory for generated data\n",
        "(pathlib.Path.cwd() / \"tmp\").mkdir(exist_ok=True)\n",
        "\n",
        "data_path = pathlib.Path.cwd() / \"tmp\" / \"data\"\n",
        "assets_directory = pathlib.Path.cwd() / \"assets\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download and extract MNIST dataset\n",
        "==================================\n",
        "\n",
        "This section downloads (if needed) the **MNIST dataset** using the `torchvision library\n",
        "<https://pytorch.org/vision/stable/index.html>`__.\n",
        "It extracts the images from the raw files and locally create two folders: one for each organization.\n",
        "\n",
        "Each organization will have access to half the train data, and to half the test data (which correspond to **30,000**\n",
        "images for training and **5,000** for testing each).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to c:\\Users\\Trabalho\\Documents\\Git\\fl-exp\\substrafl_fedavg_assets\\tmp\\data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "9913344it [00:05, 1779830.83it/s]                             \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting c:\\Users\\Trabalho\\Documents\\Git\\fl-exp\\substrafl_fedavg_assets\\tmp\\data\\MNIST\\raw\\train-images-idx3-ubyte.gz to c:\\Users\\Trabalho\\Documents\\Git\\fl-exp\\substrafl_fedavg_assets\\tmp\\data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to c:\\Users\\Trabalho\\Documents\\Git\\fl-exp\\substrafl_fedavg_assets\\tmp\\data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "29696it [00:00, 15121288.28it/s]         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting c:\\Users\\Trabalho\\Documents\\Git\\fl-exp\\substrafl_fedavg_assets\\tmp\\data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to c:\\Users\\Trabalho\\Documents\\Git\\fl-exp\\substrafl_fedavg_assets\\tmp\\data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to c:\\Users\\Trabalho\\Documents\\Git\\fl-exp\\substrafl_fedavg_assets\\tmp\\data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1649664it [00:00, 2626396.74it/s]                             \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting c:\\Users\\Trabalho\\Documents\\Git\\fl-exp\\substrafl_fedavg_assets\\tmp\\data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to c:\\Users\\Trabalho\\Documents\\Git\\fl-exp\\substrafl_fedavg_assets\\tmp\\data\\MNIST\\raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to c:\\Users\\Trabalho\\Documents\\Git\\fl-exp\\substrafl_fedavg_assets\\tmp\\data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5120it [00:00, ?it/s]                   "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting c:\\Users\\Trabalho\\Documents\\Git\\fl-exp\\substrafl_fedavg_assets\\tmp\\data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to c:\\Users\\Trabalho\\Documents\\Git\\fl-exp\\substrafl_fedavg_assets\\tmp\\data\\MNIST\\raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def get_int(b: bytes) -> int:\n",
        "    return int(codecs.encode(b, \"hex\"), 16)\n",
        "\n",
        "\n",
        "def MNISTraw2numpy(path: str, strict: bool = True) -> np.array:\n",
        "    # read\n",
        "    with open(path, \"rb\") as f:\n",
        "        data = f.read()\n",
        "    # parse\n",
        "    magic = get_int(data[0:4])\n",
        "    nd = magic % 256\n",
        "    assert 1 <= nd <= 3\n",
        "    numpy_type = np.uint8\n",
        "    s = [get_int(data[4 * (i + 1) : 4 * (i + 2)]) for i in range(nd)]\n",
        "\n",
        "    num_bytes_per_value = np.iinfo(numpy_type).bits // 8\n",
        "    # The MNIST format uses the big endian byte order. If the system uses little endian byte order by default,\n",
        "    # we need to reverse the bytes before we can read them with np.frombuffer().\n",
        "    needs_byte_reversal = sys.byteorder == \"little\" and num_bytes_per_value > 1\n",
        "    parsed = np.frombuffer(bytearray(data), dtype=numpy_type, offset=(4 * (nd + 1)))\n",
        "    if needs_byte_reversal:\n",
        "        parsed = parsed.flip(0)\n",
        "\n",
        "    assert parsed.shape[0] == np.prod(s) or not strict\n",
        "    return parsed.reshape(*s)\n",
        "\n",
        "\n",
        "raw_path = pathlib.Path(data_path) / \"MNIST\" / \"raw\"\n",
        "\n",
        "# Download the dataset\n",
        "MNIST(data_path, download=True)\n",
        "\n",
        "# Extract numpy array from raw data\n",
        "train_images = MNISTraw2numpy(str(raw_path / \"train-images-idx3-ubyte\"))\n",
        "train_labels = MNISTraw2numpy(str(raw_path / \"train-labels-idx1-ubyte\"))\n",
        "test_images = MNISTraw2numpy(str(raw_path / \"t10k-images-idx3-ubyte\"))\n",
        "test_labels = MNISTraw2numpy(str(raw_path / \"t10k-labels-idx1-ubyte\"))\n",
        "\n",
        "# Split array into the number of organization\n",
        "train_images_folds = np.split(train_images, N_CLIENTS)\n",
        "train_labels_folds = np.split(train_labels, N_CLIENTS)\n",
        "test_images_folds = np.split(test_images, N_CLIENTS)\n",
        "test_labels_folds = np.split(test_labels, N_CLIENTS)\n",
        "\n",
        "# Save splits in different folders to simulate the different organization\n",
        "for i in range(N_CLIENTS):\n",
        "\n",
        "    # Save train dataset on each org\n",
        "    os.makedirs(str(data_path / f\"org_{i+1}/train\"), exist_ok=True)\n",
        "    filename = data_path / f\"org_{i+1}/train/train_images.npy\"\n",
        "    np.save(str(filename), train_images_folds[i])\n",
        "    filename = data_path / f\"org_{i+1}/train/train_labels.npy\"\n",
        "    np.save(str(filename), train_labels_folds[i])\n",
        "\n",
        "    # Save test dataset on each org\n",
        "    os.makedirs(str(data_path / f\"org_{i+1}/test\"), exist_ok=True)\n",
        "    filename = data_path / f\"org_{i+1}/test/test_images.npy\"\n",
        "    np.save(str(filename), test_images_folds[i])\n",
        "    filename = data_path / f\"org_{i+1}/test/test_labels.npy\"\n",
        "    np.save(str(filename), test_labels_folds[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Registering assets\n",
        "******************\n",
        "\n",
        "Substra and Substrafl imports\n",
        "==============================\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from substra.sdk.schemas import (\n",
        "    DatasetSpec,\n",
        "    AlgoInputSpec,\n",
        "    AlgoOutputSpec,\n",
        "    AssetKind,\n",
        "    Permissions,\n",
        "    DataSampleSpec,\n",
        "    # AlgoCategory,\n",
        "    AlgoSpec,\n",
        ")\n",
        "from substrafl.nodes import TestDataNode, TrainDataNode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Permissions\n",
        "===========\n",
        "\n",
        "As data can not be seen once it is registered on the platform, we set `documentation/concepts:Permissions` for\n",
        "each `documentation/concepts:Assets` define their access rights to the different data.\n",
        "\n",
        "The metadata are visible by all the users of a :term:`Channel`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "permissions = Permissions(public=False, authorized_ids=ORGS_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Registering dataset\n",
        "===================\n",
        "\n",
        "A `documentation/concepts:Dataset` is composed of an **opener**, which is a Python script with the instruction\n",
        "of *how to load the data* from the files in memory, and a **description markdown** file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = DatasetSpec(\n",
        "    name=\"MNIST\",\n",
        "    type=\"npy\",\n",
        "    data_opener=assets_directory / \"dataset\" / \"opener.py\",\n",
        "    description=assets_directory / \"dataset\" / \"description.md\",\n",
        "    permissions=permissions,\n",
        "    logs_permission=permissions,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding Metrics\n",
        "==============\n",
        "\n",
        "A metric corresponds to an algorithm used to compute the score of predictions on a\n",
        "**datasample**.\n",
        "Concretely, a metric corresponds to an archive *(tar or zip file)*, automatically build\n",
        "from:\n",
        "\n",
        "- a **Python scripts** that implement the metric computation\n",
        "- a `Dockerfile <https://docs.docker.com/engine/reference/builder/>`__ to specify the required dependencies of the\n",
        "  **Python scripts**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inputs_metrics = [\n",
        "    AlgoInputSpec(\n",
        "        identifier=\"datasamples\",\n",
        "        kind=AssetKind.data_sample,\n",
        "        optional=False,\n",
        "        multiple=True,\n",
        "    ),\n",
        "    AlgoInputSpec(\n",
        "        identifier=\"opener\", kind=AssetKind.data_manager, optional=False, multiple=False\n",
        "    ),\n",
        "    AlgoInputSpec(\n",
        "        identifier=\"predictions\", kind=AssetKind.model, optional=False, multiple=False\n",
        "    ),\n",
        "]\n",
        "\n",
        "outputs_metrics = [\n",
        "    AlgoOutputSpec(identifier=\"performance\", kind=AssetKind.performance, multiple=False)\n",
        "]\n",
        "\n",
        "objective = AlgoSpec(\n",
        "    inputs=inputs_metrics,\n",
        "    outputs=outputs_metrics,\n",
        "    name=\"Accuracy\",\n",
        "    description=assets_directory / \"metric\" / \"description.md\",\n",
        "    file=assets_directory / \"metric\" / \"metrics.zip\",\n",
        "    permissions=permissions,\n",
        ")\n",
        "\n",
        "METRICS_DOCKERFILE_FILES = [\n",
        "    assets_directory / \"metric\" / \"metrics.py\",\n",
        "    assets_directory / \"metric\" / \"Dockerfile\",\n",
        "]\n",
        "\n",
        "archive_path = objective.file\n",
        "with zipfile.ZipFile(archive_path, \"w\") as z:\n",
        "    for filepath in METRICS_DOCKERFILE_FILES:\n",
        "        z.write(filepath, arcname=filepath.name)\n",
        "\n",
        "metric_key = clients[ALGO_ORG_ID].add_algo(objective)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train and test data nodes\n",
        "=========================\n",
        "\n",
        "The `documentation/concepts:Dataset` object itself does not contain the data. The proper asset to access them\n",
        "is the **datasample asset**.\n",
        "\n",
        "A **datasample** contains a local path to the data, and the key identifying the `documentation/concepts:Dataset`\n",
        "it is based on, in order to have access to the proper `opener.py` file.\n",
        "\n",
        "Now that all our `documentation/concepts:Assets` are well defined, we can create\n",
        "`substrafl_doc/api/nodes:TrainDataNode` and `substrafl_doc/api/nodes:TestDataNode` to gathered the\n",
        "`documentation/concepts:Dataset` and the **datasamples** on the specified nodes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "train_data_nodes = list()\n",
        "test_data_nodes = list()\n",
        "\n",
        "for ind, org_id in enumerate(ORGS_ID):\n",
        "    client = clients[org_id]\n",
        "\n",
        "    # Add the dataset to the client to provide access to the opener in each organization.\n",
        "    dataset_key = client.add_dataset(dataset)\n",
        "    assert dataset_key, \"Missing data manager key\"\n",
        "\n",
        "    # Add the training data on each organization.\n",
        "    data_sample = DataSampleSpec(\n",
        "        data_manager_keys=[dataset_key],\n",
        "        test_only=False,\n",
        "        path=data_path / f\"org_{ind+1}\" / \"train\",\n",
        "    )\n",
        "    train_datasample_key = client.add_data_sample(\n",
        "        data_sample,\n",
        "        local=True,\n",
        "    )\n",
        "\n",
        "    # Create the Train Data Node (or training task) and save it in a list\n",
        "    train_data_node = TrainDataNode(\n",
        "        organization_id=org_id,\n",
        "        data_manager_key=dataset_key,\n",
        "        data_sample_keys=[train_datasample_key],\n",
        "    )\n",
        "    train_data_nodes.append(train_data_node)\n",
        "\n",
        "    # Add the testing data on each organization.\n",
        "    data_sample = DataSampleSpec(\n",
        "        data_manager_keys=[dataset_key],\n",
        "        test_only=True,\n",
        "        path=data_path / f\"org_{ind+1}\" / \"test\",\n",
        "    )\n",
        "    test_datasample_key = client.add_data_sample(\n",
        "        data_sample,\n",
        "        local=True,\n",
        "    )\n",
        "\n",
        "    # Create the Test Data Node (or testing task) and save it in a list\n",
        "    test_data_node = TestDataNode(\n",
        "        organization_id=org_id,\n",
        "        data_manager_key=dataset_key,\n",
        "        test_data_sample_keys=[test_datasample_key],\n",
        "        metric_keys=[metric_key],\n",
        "    )\n",
        "    test_data_nodes.append(test_data_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Machine Learning specification\n",
        "******************************\n",
        "\n",
        "Torch imports\n",
        "=============\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "CNN definition\n",
        "==============\n",
        "\n",
        "We choose to use a classic torch CNN as the model to train. The model structure is defined by the user independently\n",
        "of Substrafl.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=5)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(3 * 3 * 64, 256)\n",
        "        self.fc2 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x, eval=False):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
        "        x = F.dropout(x, p=0.5, training=not eval)\n",
        "        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n",
        "        x = F.dropout(x, p=0.5, training=not eval)\n",
        "        x = x.view(-1, 3 * 3 * 64)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, p=0.5, training=not eval)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "\n",
        "model = CNN()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Substrafl imports\n",
        "==================\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "from substrafl.algorithms.pytorch import TorchFedAvgAlgo\n",
        "from substrafl.dependency import Dependency\n",
        "from substrafl.strategies import FedAvg\n",
        "from substrafl.nodes import AggregationNode\n",
        "from substrafl.evaluation_strategy import EvaluationStrategy\n",
        "from substrafl.index_generator import NpIndexGenerator\n",
        "from substrafl.experiment import execute_experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Substrafl algo definition\n",
        "==========================\n",
        "\n",
        "To instantiate a Substrafl `substrafl_doc/api/algorithms:Torch Algorithms`, you need to define a torch Dataset\n",
        "with a specific `__init__` signature, that must contain (self, x, y, is_inference). This torch Dataset is useful to\n",
        "preprocess your data on the `__getitem__` function.\n",
        "The `__getitem__` function is expected to return x and y if is_inference is False, else x.\n",
        "This behavior can be changed by re-writing the `_local_train` or `predict` methods.\n",
        "\n",
        "This dataset is passed **as a class** to the `substrafl_doc/api/algorithms:Torch Algorithms`.\n",
        "Indeed, this torch Dataset will be instantiated within the algorithm, using the opener functions as x and y\n",
        "parameters.\n",
        "\n",
        "The index generator will be used a the batch sampler of the dataset, in order to save the state of the seen samples\n",
        "during the training, as Federated Algorithms have a notion of `num_updates`, which forced the batch sampler of the\n",
        "dataset to be stateful.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Number of model update between each FL strategy aggregation.\n",
        "NUM_UPDATES = 100\n",
        "\n",
        "# Number of samples per update.\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "index_generator = NpIndexGenerator(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_updates=NUM_UPDATES,\n",
        ")\n",
        "\n",
        "\n",
        "class TorchDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, datasamples, is_inference: bool):\n",
        "        self.x = torch.FloatTensor(datasamples[\"images\"][:, None, ...])\n",
        "        self.y = F.one_hot(\n",
        "            torch.from_numpy(datasamples[\"labels\"]).type(torch.int64), 10\n",
        "        ).type(torch.float32)\n",
        "        self.is_inference = is_inference\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if not self.is_inference:\n",
        "            return self.x[idx] / 255, self.y[idx]\n",
        "        else:\n",
        "            return self.x[idx] / 255\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "\n",
        "class MyAlgo(TorchFedAvgAlgo):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            model=model,\n",
        "            criterion=criterion,\n",
        "            optimizer=optimizer,\n",
        "            index_generator=index_generator,\n",
        "            dataset=TorchDataset,\n",
        "            seed=seed,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Algo dependencies\n",
        "=================\n",
        "\n",
        "The **dependencies** needed for the `substrafl_doc/api/algorithms:Torch Algorithms` are specified by a\n",
        "`substrafl_doc/api/dependency:Dependency` object, in order to install the right library in the Python\n",
        "environment of each organization.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "algo_deps = Dependency(pypi_dependencies=[\"numpy==1.23.1\", \"torch==1.11.0\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Federated Learning strategies\n",
        "=============================\n",
        "\n",
        "For this example, we choose to use the **Federated averaging Strategy** (`substrafl_doc/api/strategies:Strategies`),\n",
        "based on `the FedAvg paper by McMahan et al., 2017 <https://arxiv.org/abs/1602.05629>`__.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "strategy = FedAvg()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the experiment\n",
        "**********************\n",
        "\n",
        "We now have all the necessary objects to launch our experiment. Below a summary of all the objects we created so far:\n",
        "\n",
        "- A `documentation/references/sdk:Client` to orchestrate all the assets of our project, using their keys to\n",
        "  identify them\n",
        "- An `substrafl_doc/api/algorithms:Torch Algorithms`, to define the training parameters *(optimizer, train function,\n",
        "  predict function, etc...)*\n",
        "- A `substrafl_doc/api/strategies:Strategies`, to specify the federated learning aggregation operation\n",
        "- `substrafl_doc/api/nodes:TrainDataNode`, to indicate where we can process training task, on which data and using\n",
        "  which *opener*\n",
        "- An `substrafl_doc/api/evaluation_strategy:Evaluation Strategy`, to define where and at which frequency we\n",
        "  evaluate the model\n",
        "- An `substrafl_doc/api/nodes:AggregationNode`, to specify the node on which the aggregation operation will be\n",
        "  computed\n",
        "- The **number of round**, a round being defined by a local training step followed by an aggregation operation\n",
        "- An **experiment folder** to save a summary of the operation made\n",
        "- The `substrafl_doc/api/dependency:Dependency` to define the libraries the experiment needs to run.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-11-22 22:01:54,106 - INFO - Building the compute plan.\n",
            "2022-11-22 22:01:54,109 - INFO - Registering the algorithm to Substra.\n",
            "2022-11-22 22:01:54,281 - INFO - Registering the compute plan to Substra.\n",
            "2022-11-22 22:01:54,283 - INFO - Experiment summary saved to c:\\Users\\Trabalho\\Documents\\Git\\fl-exp\\substrafl_fedavg_assets\\tmp\\experiment_summaries\\2022_11_22_22_01_54_044d4bd8-8b7e-4413-88fa-d12a4d00ff7d.json\n",
            "Compute plan progress:   0%|          | 0/23 [00:00<?, ?it/s]c:\\Users\\Trabalho\\miniconda3\\envs\\fl\\lib\\site-packages\\substra\\sdk\\backends\\local\\backend.py:885: UserWarning: `transient=True` is ignored in local mode\n",
            "  warnings.warn(\"`transient=True` is ignored in local mode\")\n",
            "Compute plan progress:   0%|          | 0/23 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "Error",
          "evalue": "[('c:\\\\Users\\\\Trabalho\\\\Documents\\\\Git\\\\fl-exp\\\\substrafl_fedavg_assets\\\\tmp\\\\data\\\\org_1\\\\train\\\\train_images.npy', 'c:\\\\Users\\\\Trabalho\\\\Documents\\\\Git\\\\fl-exp\\\\substrafl_fedavg_assets\\\\local-worker\\\\637501e3-9cb5-45ee-a430-81bfe3119fbd\\\\inputs\\\\8157a7c1-861e-4bbc-b4d6-1187038632a3\\\\train_images.npy', \"[WinError 1314] Um privilégio necessário não é mantido pelo cliente: 'c:\\\\\\\\Users\\\\\\\\Trabalho\\\\\\\\Documents\\\\\\\\Git\\\\\\\\fl-exp\\\\\\\\substrafl_fedavg_assets\\\\\\\\tmp\\\\\\\\data\\\\\\\\org_1\\\\\\\\train\\\\\\\\train_images.npy' -> 'c:\\\\\\\\Users\\\\\\\\Trabalho\\\\\\\\Documents\\\\\\\\Git\\\\\\\\fl-exp\\\\\\\\substrafl_fedavg_assets\\\\\\\\local-worker\\\\\\\\637501e3-9cb5-45ee-a430-81bfe3119fbd\\\\\\\\inputs\\\\\\\\8157a7c1-861e-4bbc-b4d6-1187038632a3\\\\\\\\train_images.npy'\"), ('c:\\\\Users\\\\Trabalho\\\\Documents\\\\Git\\\\fl-exp\\\\substrafl_fedavg_assets\\\\tmp\\\\data\\\\org_1\\\\train\\\\train_labels.npy', 'c:\\\\Users\\\\Trabalho\\\\Documents\\\\Git\\\\fl-exp\\\\substrafl_fedavg_assets\\\\local-worker\\\\637501e3-9cb5-45ee-a430-81bfe3119fbd\\\\inputs\\\\8157a7c1-861e-4bbc-b4d6-1187038632a3\\\\train_labels.npy', \"[WinError 1314] Um privilégio necessário não é mantido pelo cliente: 'c:\\\\\\\\Users\\\\\\\\Trabalho\\\\\\\\Documents\\\\\\\\Git\\\\\\\\fl-exp\\\\\\\\substrafl_fedavg_assets\\\\\\\\tmp\\\\\\\\data\\\\\\\\org_1\\\\\\\\train\\\\\\\\train_labels.npy' -> 'c:\\\\\\\\Users\\\\\\\\Trabalho\\\\\\\\Documents\\\\\\\\Git\\\\\\\\fl-exp\\\\\\\\substrafl_fedavg_assets\\\\\\\\local-worker\\\\\\\\637501e3-9cb5-45ee-a430-81bfe3119fbd\\\\\\\\inputs\\\\\\\\8157a7c1-861e-4bbc-b4d6-1187038632a3\\\\\\\\train_labels.npy'\")]",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [17], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39m# Number of time to apply the compute plan.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m NUM_ROUNDS \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m----> 8\u001b[0m compute_plan \u001b[39m=\u001b[39m execute_experiment(\n\u001b[0;32m      9\u001b[0m     client\u001b[39m=\u001b[39;49mclients[ALGO_ORG_ID],\n\u001b[0;32m     10\u001b[0m     algo\u001b[39m=\u001b[39;49mMyAlgo(),\n\u001b[0;32m     11\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[0;32m     12\u001b[0m     train_data_nodes\u001b[39m=\u001b[39;49mtrain_data_nodes,\n\u001b[0;32m     13\u001b[0m     evaluation_strategy\u001b[39m=\u001b[39;49mmy_eval_strategy,\n\u001b[0;32m     14\u001b[0m     aggregation_node\u001b[39m=\u001b[39;49maggregation_node,\n\u001b[0;32m     15\u001b[0m     num_rounds\u001b[39m=\u001b[39;49mNUM_ROUNDS,\n\u001b[0;32m     16\u001b[0m     experiment_folder\u001b[39m=\u001b[39;49m\u001b[39mstr\u001b[39;49m(pathlib\u001b[39m.\u001b[39;49mPath\u001b[39m.\u001b[39;49mcwd() \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mtmp\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m/\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mexperiment_summaries\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m     17\u001b[0m     dependencies\u001b[39m=\u001b[39;49malgo_deps,\n\u001b[0;32m     18\u001b[0m )\n",
            "File \u001b[1;32mc:\\Users\\Trabalho\\miniconda3\\envs\\fl\\lib\\site-packages\\substrafl\\experiment.py:362\u001b[0m, in \u001b[0;36mexecute_experiment\u001b[1;34m(client, algo, strategy, train_data_nodes, num_rounds, experiment_folder, aggregation_node, evaluation_strategy, dependencies, clean_models, name, additional_metadata, task_submission_batch_size)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39m# save the experiment summary in experiment_folder\u001b[39;00m\n\u001b[0;32m    349\u001b[0m _save_experiment_summary(\n\u001b[0;32m    350\u001b[0m     experiment_folder\u001b[39m=\u001b[39mexperiment_folder,\n\u001b[0;32m    351\u001b[0m     compute_plan_key\u001b[39m=\u001b[39mcompute_plan_key,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    360\u001b[0m     additional_metadata\u001b[39m=\u001b[39madditional_metadata,\n\u001b[0;32m    361\u001b[0m )\n\u001b[1;32m--> 362\u001b[0m compute_plan \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49madd_compute_plan(\n\u001b[0;32m    363\u001b[0m     substra\u001b[39m.\u001b[39;49msdk\u001b[39m.\u001b[39;49mschemas\u001b[39m.\u001b[39;49mComputePlanSpec(\n\u001b[0;32m    364\u001b[0m         key\u001b[39m=\u001b[39;49mcompute_plan_key,\n\u001b[0;32m    365\u001b[0m         composite_traintuples\u001b[39m=\u001b[39;49mcomposite_traintuples,\n\u001b[0;32m    366\u001b[0m         aggregatetuples\u001b[39m=\u001b[39;49maggregation_tuples,\n\u001b[0;32m    367\u001b[0m         predicttuples\u001b[39m=\u001b[39;49mpredicttuples,\n\u001b[0;32m    368\u001b[0m         testtuples\u001b[39m=\u001b[39;49mtesttuples,\n\u001b[0;32m    369\u001b[0m         name\u001b[39m=\u001b[39;49mname \u001b[39mor\u001b[39;49;00m timestamp,\n\u001b[0;32m    370\u001b[0m         metadata\u001b[39m=\u001b[39;49mcp_metadata,\n\u001b[0;32m    371\u001b[0m     ),\n\u001b[0;32m    372\u001b[0m     auto_batching\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    373\u001b[0m     batch_size\u001b[39m=\u001b[39;49mtask_submission_batch_size,\n\u001b[0;32m    374\u001b[0m )\n\u001b[0;32m    375\u001b[0m logger\u001b[39m.\u001b[39minfo((\u001b[39m\"\u001b[39m\u001b[39mThe compute plan has been registered to Substra, its key is \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mformat(compute_plan\u001b[39m.\u001b[39mkey))\n\u001b[0;32m    376\u001b[0m \u001b[39mreturn\u001b[39;00m compute_plan\n",
            "File \u001b[1;32mc:\\Users\\Trabalho\\miniconda3\\envs\\fl\\lib\\site-packages\\substra\\sdk\\client.py:39\u001b[0m, in \u001b[0;36mlogit.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m error \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     41\u001b[0m     error \u001b[39m=\u001b[39m e\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\Trabalho\\miniconda3\\envs\\fl\\lib\\site-packages\\substra\\sdk\\client.py:422\u001b[0m, in \u001b[0;36mClient.add_compute_plan\u001b[1;34m(self, data, auto_batching, batch_size)\u001b[0m\n\u001b[0;32m    416\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_valid_uuid(spec\u001b[39m.\u001b[39mkey):\n\u001b[0;32m    417\u001b[0m     \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mComputePlanKeyFormatError(\n\u001b[0;32m    418\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe compute plan key has to respect the UUID format. You can use the uuid library to generate it. \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[39m    Example: compute_plan_key=str(uuid.uuid4())\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    420\u001b[0m     )\n\u001b[1;32m--> 422\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49madd(spec, spec_options\u001b[39m=\u001b[39;49mspec_options)\n",
            "File \u001b[1;32mc:\\Users\\Trabalho\\miniconda3\\envs\\fl\\lib\\site-packages\\substra\\sdk\\backends\\local\\backend.py:749\u001b[0m, in \u001b[0;36mLocal.add\u001b[1;34m(self, spec, spec_options, key)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m spec\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39mtype_ \u001b[39m==\u001b[39m schemas\u001b[39m.\u001b[39mType\u001b[39m.\u001b[39mComputePlan:\n\u001b[1;32m--> 749\u001b[0m         compute_plan \u001b[39m=\u001b[39m add_asset(spec, spec_options)\n\u001b[0;32m    750\u001b[0m         \u001b[39mreturn\u001b[39;00m compute_plan\n\u001b[0;32m    751\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Trabalho\\miniconda3\\envs\\fl\\lib\\site-packages\\substra\\sdk\\backends\\local\\backend.py:422\u001b[0m, in \u001b[0;36mLocal._add_compute_plan\u001b[1;34m(self, spec, spec_options)\u001b[0m\n\u001b[0;32m    419\u001b[0m compute_plan \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_db\u001b[39m.\u001b[39madd(compute_plan)\n\u001b[0;32m    421\u001b[0m \u001b[39m# go through the tuples sorted by rank\u001b[39;00m\n\u001b[1;32m--> 422\u001b[0m compute_plan \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__execute_compute_plan(spec, compute_plan, visited, tuples, spec_options)\n\u001b[0;32m    423\u001b[0m \u001b[39mreturn\u001b[39;00m compute_plan\n",
            "File \u001b[1;32mc:\\Users\\Trabalho\\miniconda3\\envs\\fl\\lib\\site-packages\\substra\\sdk\\backends\\local\\backend.py:247\u001b[0m, in \u001b[0;36mLocal.__execute_compute_plan\u001b[1;34m(self, spec, compute_plan, visited, tuples, spec_options)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tuple_spec:\n\u001b[0;32m    245\u001b[0m             \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m--> 247\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd(\n\u001b[0;32m    248\u001b[0m             key\u001b[39m=\u001b[39;49mtuple_spec\u001b[39m.\u001b[39;49mkey,\n\u001b[0;32m    249\u001b[0m             spec\u001b[39m=\u001b[39;49mtuple_spec,\n\u001b[0;32m    250\u001b[0m             spec_options\u001b[39m=\u001b[39;49mspec_options,\n\u001b[0;32m    251\u001b[0m         )\n\u001b[0;32m    253\u001b[0m         progress_bar\u001b[39m.\u001b[39mupdate()\n\u001b[0;32m    255\u001b[0m \u001b[39mreturn\u001b[39;00m compute_plan\n",
            "File \u001b[1;32mc:\\Users\\Trabalho\\miniconda3\\envs\\fl\\lib\\site-packages\\substra\\sdk\\backends\\local\\backend.py:753\u001b[0m, in \u001b[0;36mLocal.add\u001b[1;34m(self, spec, spec_options, key)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    752\u001b[0m     key \u001b[39m=\u001b[39m key \u001b[39mor\u001b[39;00m spec\u001b[39m.\u001b[39mcompute_key()\n\u001b[1;32m--> 753\u001b[0m     add_asset(key, spec, spec_options)\n\u001b[0;32m    754\u001b[0m     \u001b[39mreturn\u001b[39;00m key\n",
            "File \u001b[1;32mc:\\Users\\Trabalho\\miniconda3\\envs\\fl\\lib\\site-packages\\substra\\sdk\\backends\\local\\backend.py:653\u001b[0m, in \u001b[0;36mLocal._add_composite_traintuple\u001b[1;34m(self, key, spec, spec_options)\u001b[0m\n\u001b[0;32m    651\u001b[0m composite_traintuple \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_db\u001b[39m.\u001b[39madd(composite_traintuple)\n\u001b[0;32m    652\u001b[0m \u001b[39mif\u001b[39;00m composite_traintuple\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m models\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mwaiting:\n\u001b[1;32m--> 653\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_worker\u001b[39m.\u001b[39;49mschedule_task(composite_traintuple)\n\u001b[0;32m    654\u001b[0m \u001b[39mreturn\u001b[39;00m composite_traintuple\n",
            "File \u001b[1;32mc:\\Users\\Trabalho\\miniconda3\\envs\\fl\\lib\\site-packages\\substra\\sdk\\backends\\local\\compute\\worker.py:353\u001b[0m, in \u001b[0;36mWorker.schedule_task\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[39m# Task execution\u001b[39;00m\n\u001b[0;32m    352\u001b[0m container_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39malgo-\u001b[39m\u001b[39m{\u001b[39;00malgo\u001b[39m.\u001b[39malgorithm\u001b[39m.\u001b[39mchecksum\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 353\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spawner\u001b[39m.\u001b[39;49mspawn(\n\u001b[0;32m    354\u001b[0m     container_name,\n\u001b[0;32m    355\u001b[0m     \u001b[39mstr\u001b[39;49m(algo\u001b[39m.\u001b[39;49malgorithm\u001b[39m.\u001b[39;49mstorage_address),\n\u001b[0;32m    356\u001b[0m     command_args_tpl\u001b[39m=\u001b[39;49m[string\u001b[39m.\u001b[39;49mTemplate(\u001b[39mstr\u001b[39;49m(part)) \u001b[39mfor\u001b[39;49;00m part \u001b[39min\u001b[39;49;00m command_template],\n\u001b[0;32m    357\u001b[0m     local_volumes\u001b[39m=\u001b[39;49mvolumes,\n\u001b[0;32m    358\u001b[0m     data_sample_paths\u001b[39m=\u001b[39;49mdata_sample_paths,\n\u001b[0;32m    359\u001b[0m     envs\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    360\u001b[0m )\n\u001b[0;32m    362\u001b[0m \u001b[39m# Save the outputs\u001b[39;00m\n\u001b[0;32m    363\u001b[0m update_live_performances \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\Trabalho\\miniconda3\\envs\\fl\\lib\\site-packages\\substra\\sdk\\backends\\local\\compute\\spawner\\subprocess.py:110\u001b[0m, in \u001b[0;36mSubprocess.spawn\u001b[1;34m(self, name, archive_path, command_args_tpl, data_sample_paths, local_volumes, envs)\u001b[0m\n\u001b[0;32m    107\u001b[0m write_command_args_file(args_file, py_command_args)\n\u001b[0;32m    109\u001b[0m \u001b[39mif\u001b[39;00m data_sample_paths \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(data_sample_paths) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 110\u001b[0m     _symlink_data_samples(data_sample_paths, local_volumes[VOLUME_INPUTS])\n\u001b[0;32m    112\u001b[0m \u001b[39m# Catching error and raising to be ISO to the docker local backend\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[39m# Don't capture the output to be able to use pdb\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\Trabalho\\miniconda3\\envs\\fl\\lib\\site-packages\\substra\\sdk\\backends\\local\\compute\\spawner\\subprocess.py:76\u001b[0m, in \u001b[0;36m_symlink_data_samples\u001b[1;34m(data_sample_paths, dest_dir)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mfor\u001b[39;00m sample_key, sample_path \u001b[39min\u001b[39;00m data_sample_paths\u001b[39m.\u001b[39mitems():\n\u001b[0;32m     75\u001b[0m     dest_path \u001b[39m=\u001b[39m pathlib\u001b[39m.\u001b[39mPath(dest_dir) \u001b[39m/\u001b[39m sample_key\n\u001b[1;32m---> 76\u001b[0m     shutil\u001b[39m.\u001b[39;49mcopytree(sample_path, dest_path, copy_function\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49msymlink)\n",
            "File \u001b[1;32mc:\\Users\\Trabalho\\miniconda3\\envs\\fl\\lib\\shutil.py:568\u001b[0m, in \u001b[0;36mcopytree\u001b[1;34m(src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m os\u001b[39m.\u001b[39mscandir(src) \u001b[39mas\u001b[39;00m itr:\n\u001b[0;32m    567\u001b[0m     entries \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(itr)\n\u001b[1;32m--> 568\u001b[0m \u001b[39mreturn\u001b[39;00m _copytree(entries\u001b[39m=\u001b[39;49mentries, src\u001b[39m=\u001b[39;49msrc, dst\u001b[39m=\u001b[39;49mdst, symlinks\u001b[39m=\u001b[39;49msymlinks,\n\u001b[0;32m    569\u001b[0m                  ignore\u001b[39m=\u001b[39;49mignore, copy_function\u001b[39m=\u001b[39;49mcopy_function,\n\u001b[0;32m    570\u001b[0m                  ignore_dangling_symlinks\u001b[39m=\u001b[39;49mignore_dangling_symlinks,\n\u001b[0;32m    571\u001b[0m                  dirs_exist_ok\u001b[39m=\u001b[39;49mdirs_exist_ok)\n",
            "File \u001b[1;32mc:\\Users\\Trabalho\\miniconda3\\envs\\fl\\lib\\shutil.py:522\u001b[0m, in \u001b[0;36m_copytree\u001b[1;34m(entries, src, dst, symlinks, ignore, copy_function, ignore_dangling_symlinks, dirs_exist_ok)\u001b[0m\n\u001b[0;32m    520\u001b[0m         errors\u001b[39m.\u001b[39mappend((src, dst, \u001b[39mstr\u001b[39m(why)))\n\u001b[0;32m    521\u001b[0m \u001b[39mif\u001b[39;00m errors:\n\u001b[1;32m--> 522\u001b[0m     \u001b[39mraise\u001b[39;00m Error(errors)\n\u001b[0;32m    523\u001b[0m \u001b[39mreturn\u001b[39;00m dst\n",
            "\u001b[1;31mError\u001b[0m: [('c:\\\\Users\\\\Trabalho\\\\Documents\\\\Git\\\\fl-exp\\\\substrafl_fedavg_assets\\\\tmp\\\\data\\\\org_1\\\\train\\\\train_images.npy', 'c:\\\\Users\\\\Trabalho\\\\Documents\\\\Git\\\\fl-exp\\\\substrafl_fedavg_assets\\\\local-worker\\\\637501e3-9cb5-45ee-a430-81bfe3119fbd\\\\inputs\\\\8157a7c1-861e-4bbc-b4d6-1187038632a3\\\\train_images.npy', \"[WinError 1314] Um privilégio necessário não é mantido pelo cliente: 'c:\\\\\\\\Users\\\\\\\\Trabalho\\\\\\\\Documents\\\\\\\\Git\\\\\\\\fl-exp\\\\\\\\substrafl_fedavg_assets\\\\\\\\tmp\\\\\\\\data\\\\\\\\org_1\\\\\\\\train\\\\\\\\train_images.npy' -> 'c:\\\\\\\\Users\\\\\\\\Trabalho\\\\\\\\Documents\\\\\\\\Git\\\\\\\\fl-exp\\\\\\\\substrafl_fedavg_assets\\\\\\\\local-worker\\\\\\\\637501e3-9cb5-45ee-a430-81bfe3119fbd\\\\\\\\inputs\\\\\\\\8157a7c1-861e-4bbc-b4d6-1187038632a3\\\\\\\\train_images.npy'\"), ('c:\\\\Users\\\\Trabalho\\\\Documents\\\\Git\\\\fl-exp\\\\substrafl_fedavg_assets\\\\tmp\\\\data\\\\org_1\\\\train\\\\train_labels.npy', 'c:\\\\Users\\\\Trabalho\\\\Documents\\\\Git\\\\fl-exp\\\\substrafl_fedavg_assets\\\\local-worker\\\\637501e3-9cb5-45ee-a430-81bfe3119fbd\\\\inputs\\\\8157a7c1-861e-4bbc-b4d6-1187038632a3\\\\train_labels.npy', \"[WinError 1314] Um privilégio necessário não é mantido pelo cliente: 'c:\\\\\\\\Users\\\\\\\\Trabalho\\\\\\\\Documents\\\\\\\\Git\\\\\\\\fl-exp\\\\\\\\substrafl_fedavg_assets\\\\\\\\tmp\\\\\\\\data\\\\\\\\org_1\\\\\\\\train\\\\\\\\train_labels.npy' -> 'c:\\\\\\\\Users\\\\\\\\Trabalho\\\\\\\\Documents\\\\\\\\Git\\\\\\\\fl-exp\\\\\\\\substrafl_fedavg_assets\\\\\\\\local-worker\\\\\\\\637501e3-9cb5-45ee-a430-81bfe3119fbd\\\\\\\\inputs\\\\\\\\8157a7c1-861e-4bbc-b4d6-1187038632a3\\\\\\\\train_labels.npy'\")]"
          ]
        }
      ],
      "source": [
        "aggregation_node = AggregationNode(ALGO_ORG_ID)\n",
        "\n",
        "my_eval_strategy = EvaluationStrategy(test_data_nodes=test_data_nodes, rounds=1)\n",
        "\n",
        "# Number of time to apply the compute plan.\n",
        "NUM_ROUNDS = 3\n",
        "\n",
        "compute_plan = execute_experiment(\n",
        "    client=clients[ALGO_ORG_ID],\n",
        "    algo=MyAlgo(),\n",
        "    strategy=strategy,\n",
        "    train_data_nodes=train_data_nodes,\n",
        "    evaluation_strategy=my_eval_strategy,\n",
        "    aggregation_node=aggregation_node,\n",
        "    num_rounds=NUM_ROUNDS,\n",
        "    experiment_folder=str(pathlib.Path.cwd() / \"tmp\" / \"experiment_summaries\"),\n",
        "    dependencies=algo_deps,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Listing results\n",
        "===============\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "performances_df = pd.DataFrame(client.get_performances(compute_plan.key).dict())\n",
        "print(\"\\nPerformance Table: \\n\")\n",
        "print(performances_df[[\"worker\", \"round_idx\", \"performance\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot results\n",
        "============\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.title(\"Test dataset results\")\n",
        "plt.xlabel(\"Rounds\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "for id in ORGS_ID:\n",
        "    df = performances_df.query(f\"worker == '{id}'\")\n",
        "    plt.plot(df[\"round_idx\"], df[\"performance\"], label=id)\n",
        "\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.9.15 ('fl')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "4184aae088894a3740fc47dfcddf6bb95213e93e54e8cc72e0849571dc8d4493"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
